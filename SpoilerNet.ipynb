{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpoilerNet",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Importing required modules\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_curve\n",
        "from sklearn.metrics import auc"
      ],
      "metadata": {
        "id": "UQttty-9FzG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Reading dataset\n",
        "\n",
        "full_df = pd.read_csv(\"/content/drive/MyDrive/cleaned_reviews_summaries.zip\")"
      ],
      "metadata": {
        "id": "hxO7NuZDSz1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating vocab and embeddings"
      ],
      "metadata": {
        "id": "jM6XwVQxhBC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch"
      ],
      "metadata": {
        "id": "imvQRCTgg_fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_df.head()"
      ],
      "metadata": {
        "id": "L8Syq8OShHle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_df = full_df.sample(100000)"
      ],
      "metadata": {
        "id": "SH1DpYN3JuDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()\n",
        "word_to_id = dict()\n",
        "id_to_word = dict()\n",
        "word_to_count = dict()\n",
        "vocab_size = 1"
      ],
      "metadata": {
        "id": "VB_zs9syhop8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 30 # placeholder value"
      ],
      "metadata": {
        "id": "Okgsyx8uhrpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['cleaned_reviews'] # Use if you want vocab to include just review text"
      ],
      "metadata": {
        "id": "PAwr9I75ht6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate vocab"
      ],
      "metadata": {
        "id": "8z5O5GiHh4oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_vocab(cols):\n",
        "    \n",
        "    global vocab_size, vocab\n",
        "    global word_to_id, id_to_word, word_to_count\n",
        "    global full_df\n",
        "    global df_mini\n",
        "    cleaned_reviews_ids = []\n",
        "    cleaned_summaries_ids = []\n",
        "    \n",
        "    for index, row in full_df.iterrows():\n",
        "        \n",
        "        for c in cols:\n",
        "            \n",
        "            s2n = []\n",
        "            split = row[c].split()\n",
        "            \n",
        "            for word in split:\n",
        "                \n",
        "                if word not in vocab:\n",
        "                    vocab.add(word)\n",
        "                    word_to_id[word] = vocab_size\n",
        "                    # word_to_count[word] = 1\n",
        "                    s2n.append(vocab_size)\n",
        "                    id_to_word[vocab_size] = word\n",
        "                    vocab_size += 1\n",
        "                    \n",
        "                else:\n",
        "                    # word_to_count[word] += 1\n",
        "                    s2n.append(word_to_id[word])\n",
        "                \n",
        "\n",
        "            full_df.at[index, c] = s2n\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "RkGZqSAxhxKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time as time"
      ],
      "metadata": {
        "id": "Ab4gpUUOiAbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes about 100 seconds with just cleaned reviews column, will take longer if you include cleaned summaries\n",
        "start = time.time()\n",
        "generate_vocab(cols)\n",
        "print(time.time() - start)"
      ],
      "metadata": {
        "id": "01SqJbzth97g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Item Specificivity DF and IIF"
      ],
      "metadata": {
        "id": "Qw0ASzESN0hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def convert_movie_ids():\n",
        "\n",
        "#   movie_to_idx = {}\n",
        "#   new_id = 0\n",
        "#   for index, row in full_df.iterrows():\n",
        "#     key = row['movie_id']\n",
        "#     # if key in movie_to_idx:\n",
        "#       full_df.at[index, 'movie_id'] = movie_to_idx[key]\n",
        "#     else:\n",
        "#       full_df.at[index, 'movie_id'] = new_id\n",
        "#       movie_to_idx[key] = new_id\n",
        "#       new_id += 1\n"
      ],
      "metadata": {
        "id": "D0mOL1NH--qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_movie_ids()"
      ],
      "metadata": {
        "id": "SrxWgmc39ubH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_movies = full_df.movie_id.unique().shape[0]"
      ],
      "metadata": {
        "id": "9GPMTvOJ_pZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d = np.zeros(num_movies)\n",
        "# dw_i = np.zeros((vocab_size, num_movies))\n",
        "# # dw_i = {}\n",
        "# for index, row in full_df.iterrows():\n",
        "#   item = row['movie_id']\n",
        "#   # d[item] = d.get(item, 0) + 1 # number of reviews for movie i\n",
        "#   d[item] += 1\n",
        "#   words = row['cleaned_reviews']\n",
        "#   movie_id = row['movie_id']\n",
        "#   seen = set()\n",
        "#   for word in words:\n",
        "#     if word not in seen:\n",
        "#       seen.add(word)\n",
        "#       # key = str(word) + str(movie_id)\n",
        "#       dw_i[word][item] += 1 # number of reveiws of movie i that contain given word\n",
        "#       # dw_i[key] = dw_i.get(key, 0) + 1\n"
      ],
      "metadata": {
        "id": "mgZ1FyKRAp7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dw_i.shape, d.shape"
      ],
      "metadata": {
        "id": "duiSnVUMJW6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DF = dw_i/d # (vocab_size, num_movies)"
      ],
      "metadata": {
        "id": "IwuSHvp-Ke8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Iw = np.count_nonzero(dw_i, axis = 1)\n",
        "# I = num_movies\n",
        "# IIF = np.log((I+1)/(Iw + 1)) # (vocab_size,)"
      ],
      "metadata": {
        "id": "1qYOxvcTMGEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(DF.shape, IIF.shape)  "
      ],
      "metadata": {
        "id": "TmlLtme3NF0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(vocab))"
      ],
      "metadata": {
        "id": "qxh2G_baiDLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train, Test, Val Split"
      ],
      "metadata": {
        "id": "bHtj2Xh3OjSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "X_val = []\n",
        "y_val = []"
      ],
      "metadata": {
        "id": "Bd4NQVRIhxQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_df['is_spoiler'] = full_df['is_spoiler'].fillna(0)\n",
        "full_df['is_spoiler'].isna().sum()"
      ],
      "metadata": {
        "id": "magc9LOOihEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(cols):\n",
        "    \n",
        "    global X_train, y_train\n",
        "    global X_test, y_test\n",
        "    global X_val, y_val\n",
        "    global X_train_movie_ids, X_test_movie_ids, X_val_movie_ids\n",
        "    \n",
        "    train_ratio = 0.8 # Split into training and validation\n",
        "    test_ratio = 0.85 # Split into training and testing\n",
        "    \n",
        "    X = list(full_df['cleaned_reviews'])\n",
        "    y = list(full_df['is_spoiler'].astype(int))\n",
        "    \n",
        "    X_rem, X_test, y_rem, y_test = train_test_split(X, y, train_size=test_ratio)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_rem, y_rem, train_size=train_ratio)\n",
        "    \n",
        "    \n",
        "    # X_train = [X_train[c] for c in cols]\n",
        "    # X_val = [X_val[c] for c in cols]\n",
        "    # X_test = [X_test[c] for c in cols]\n",
        "    \n",
        "    # y_train = y_train.values\n",
        "    # y_test = y_test.values\n",
        "    # y_val = y_val.values"
      ],
      "metadata": {
        "id": "Ba1jPsj_ijJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "split_data(cols)\n",
        "print(time.time() - start)"
      ],
      "metadata": {
        "id": "vu_oRsTIinPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train), len(y_train), len(X_test), len(y_test))"
      ],
      "metadata": {
        "id": "qs_4EbTEip-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences(X_list):\n",
        "    \n",
        "    X_padded = torch.nn.utils.rnn.pad_sequence([torch.as_tensor(l) for l in X_list]).type(torch.LongTensor) # padding the sequences with 0\n",
        "    X_mask   = torch.nn.utils.rnn.pad_sequence([torch.as_tensor([1.0] * len(l)) for l in X_list]).type(torch.FloatTensor)\n",
        "    \n",
        "    return X_padded, X_mask"
      ],
      "metadata": {
        "id": "jUqNAid0iud-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes about 20 seconds to run\n",
        "start = time.time()\n",
        "X_train = pad_sequences(X_train)[0]\n",
        "print(time.time() - start)"
      ],
      "metadata": {
        "id": "AJu3Bam8iwv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pad_sequences(X_test)[0]\n",
        "X_val = pad_sequences(X_val)[0]"
      ],
      "metadata": {
        "id": "LFd-59mIizVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train = torch.Tensor(y_train)\n",
        "# y_test = torch.Tensor(y_test)\n",
        "# y_val = torch.Tensor(y_val)"
      ],
      "metadata": {
        "id": "chDcGNu2jvuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape # [max_sentence_len, num_data_points]"
      ],
      "metadata": {
        "id": "pzXip3eVNVOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.T\n",
        "X_test = X_test.T\n",
        "X_val = X_val.T\n",
        "\n",
        "print(X_train.shape, X_test.shape, X_val.shape)"
      ],
      "metadata": {
        "id": "4vZJ2AlgO5cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spoiler Net"
      ],
      "metadata": {
        "id": "aM0wleT7js1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpoilerNet(torch.nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size, emb_dim = 300, hid_dim = 50):\n",
        "    super(SpoilerNet, self).__init__()\n",
        "\n",
        "    # initialize parameters\n",
        "    self.EMB_DIM = emb_dim\n",
        "    self.DIM_HIDDEN = hid_dim\n",
        "    self.VOCAB_SIZE = vocab_size\n",
        "\n",
        "    # initialize layers\n",
        "\n",
        "    self.embedding = torch.nn.Embedding(self.VOCAB_SIZE, self.EMB_DIM)\n",
        "    self.word_encoder = torch.nn.GRU(self.EMB_DIM, self.DIM_HIDDEN, bidirectional = True) \n",
        "\n",
        "    ## for word attention ##\n",
        "    self.mu = torch.nn.Linear(self.DIM_HIDDEN, self.DIM_HIDDEN)\n",
        "    self.tanh = torch.nn.Tanh()\n",
        "    self.v = torch.nn.Linear(self.DIM_HIDDEN, self.DIM_HIDDEN, bias = False)\n",
        "    self.alpha = torch.nn.Softmax(dim = 1) \n",
        "  \n",
        "    \n",
        "\n",
        "    self.sentence_encoder = torch.nn.GRU(self.DIM_HIDDEN, self.DIM_HIDDEN, bidirectional = True, batch_first = True)\n",
        "    self.dropout = torch.nn.Dropout(0.5) # according to paper \n",
        "    self.output = torch.nn.Linear(self.DIM_HIDDEN, 2)\n",
        "    self.activation = torch.nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self, X):\n",
        "  \n",
        "    text_embeddings = self.embedding(X) # generating text embeddings\n",
        "\n",
        "    word_enc_out, word_hn = self.word_encoder(text_embeddings) # word encoder outputs\n",
        "\n",
        "    h_w = word_enc_out[:, :, :self.DIM_HIDDEN] + word_enc_out[:, :, self.DIM_HIDDEN:] # adding output represen of both GRUs\n",
        "\n",
        "    \n",
        "    # applying word attention\n",
        "\n",
        "    mu_w = self.tanh(self.mu(h_w))\n",
        "    \n",
        "    v_out = self.v(mu_w)\n",
        "\n",
        "    alpha_w = self.alpha(v_out)\n",
        "\n",
        "    # input for sentence encoder\n",
        "\n",
        "    v_s = torch.sum(alpha_w * h_w, dim = 1)\n",
        "\n",
        "    sent_enc_out, _ = self.sentence_encoder(v_s) # sentence encoder outputs\n",
        "\n",
        "    h_s = sent_enc_out[:, :self.DIM_HIDDEN] + sent_enc_out[:, self.DIM_HIDDEN:] # adding output represen of both GRUs\n",
        "    \n",
        "    out = self.output(self.dropout((h_s))) # unnormalized class scores (batchSize, 2)\n",
        "\n",
        "    return out     \n",
        "  \n"
      ],
      "metadata": {
        "id": "DD5CG3POgTqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, x, y): # function for predicting\n",
        "  y_pred = []\n",
        "  num_correct = 0\n",
        "  x = x.cuda()\n",
        "  sigmoid = torch.nn.Sigmoid()\n",
        "  for i in range(len(x)):\n",
        "    probs = sigmoid(model.forward(x[i].unsqueeze(0)))\n",
        "    pred = torch.argmax(probs)\n",
        "    y_pred.append(pred.item())\n",
        "    \n",
        "    if pred == y[i]:\n",
        "      num_correct += 1\n",
        "  print(\"Accuracy: %s\" % (float(num_correct) / float(len(x))))\n",
        "  return y_pred\n",
        "\n",
        "\n",
        "## TRAINING\n",
        "\n",
        "## Parameters according to paper ##\n",
        "\n",
        "NUM_EPOCHS = 4\n",
        "batchSize = 64 \n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "model = SpoilerNet(vocab_size).cuda()\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "y_train = torch.LongTensor(y_train)\n",
        "clip = 50.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  for i in range(0, X_train.shape[0], batchSize):\n",
        "    optimizer.zero_grad()\n",
        "    s = i\n",
        "    e = i + batchSize\n",
        "    if i + batchSize >= X_train.shape[0]:\n",
        "      e = X_train.shape[0]\n",
        "\n",
        "    X_batch = X_train[s:e, :]\n",
        "    Y_batch = y_train[s:e]\n",
        "    output = model.forward(X_batch.cuda())\n",
        "    loss = loss_func(output, Y_batch.cuda())\n",
        "    total_loss += loss.item()\n",
        "    loss.backward() \n",
        "    _ = torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # gradient clipping\n",
        "    optimizer.step()\n",
        "  model.eval()\n",
        "  print(\"Loss at epoch\", epoch,\":\", total_loss, end = '\\t')\n",
        "  _ = predict(model, X_val, y_val)"
      ],
      "metadata": {
        "id": "yrOtRXteYXrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_pred = predict(model, X_test, y_test)"
      ],
      "metadata": {
        "id": "9_X1RNPfv7Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "gvkYsddMs9gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision, _, _, _ = precision_recall_fscore_support(y_test, y_pred)"
      ],
      "metadata": {
        "id": "t5_Q1cvt0CAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision.mean())"
      ],
      "metadata": {
        "id": "6QVFtDJeRqKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ROC\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "duzgimaKpkfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(fpr, tpr)\n",
        "plt.title(\"ROC\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5xXvoNRgRQDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(auc(fpr, tpr))"
      ],
      "metadata": {
        "id": "NvGQk-yRFqrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f1e6XY_N1wev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}